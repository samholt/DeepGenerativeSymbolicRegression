{
   "task" : {
      // Deep Symbolic Policy
      "task_type" : "control",

      // Name of your environment. The argument is similar to what you'd use for
      // gym.make(). To use a custom environment, use "mymodule:MyEnv-v0", where
      // MyEnv-v0 is expected to be registered to gym upon importing mymodule.
      "env" : "CustomCartPoleContinuous-v0",

      // Path to a pre-trained stable-baselines "anchor". The environments used
      // in the ICML paper have default values, so you do not have to specify
      // one. You also don't have to specify an "anchor" for environments with
      // one-dimensional action spaces.
      "anchor" : null,

      // This list specifies which action is to be learned symbolically (null),
      // which ones should use the anchor model ("anchor"), and which ones
      // should use previously learned symbolic policies (str representing the
      // expression).
      "action_spec" : [null],

      // Number of episodes to average per reward computation.
      "n_episodes_train" : 10,

      // Number of test episodes to average to determine the best policies.
      "n_episodes_test" : 1000,

      // Early stopping is triggered if all test episode's episodic rewards are
      // >= success_score.
      "success_score": 999999.0,

      // Allowed functions. See functions.py for a list of supported functions.
      // Hard-coded constants can be combined to form new constants. The use of
      // the placeholder constant "const" is supported but is likely
      // prohibitively expensive for the control task, so is not recommended.
      "function_set" : ["add", "sub", "mul", "div", "sin", "cos", "exp", "log", 0.1, 1, 5],

      // With protected=false, floating-point errors (e.g. log of negative
      // number) may occur, in which case the action is set to 0 for that
      // timestep. With protected=true, "protected" functions will prevent
      // floating-point errors, but may introduce discontinuities in the learned
      // functions.
      "protected" : false,

      // If true, each reward computation will use the same set of seeds (via
      // env.seed()). This is useful because it renders the task deterministic.
      // However, it can introduce bias if the seeds aren't representative of
      // the whole environment.
      "fix_seeds" : true,

      // If fix_seeds=True, this shifts the seeds used each episode, which
      // changes the reward function from run to run. If fix_seeds=false, this
      // has no effect.
      "episode_seed_shift" : 0,

      // If your env has kwargs, they can go here.
      "env_kwargs" : {},

      // A list of [r_min, r_max] for your environment, used to rescale rewards
      // to roughly [0, 1]. If true, it uses precomputed [r_min, r_max] for
      // environments supported in control.py. If false, reward scaling is not
      // used.
      "reward_scale" : true,

      // This sets whether or not to simultaneously train every action dimension in the environment at once.
      "multiobject": false,

      // Set of thresholds (shared by all state variables) for building
      // decision trees. Note that no StateChecker will be added to Library
      // if decision_tree_threshold_set is an empty list or null.
      "decision_tree_threshold_set" : []
   },

   // Only the key training hyperparameters are listed here. See
   // config_common.json for the full list.
   "training" : {
      "n_samples": 200000,
      "batch_size": 200,
      "epsilon": 0.1,

      // Recommended to set this to as many cores as you can use!
      "n_cores_batch" : 1,

      "early_stopping" : false
   },

   // Only the key RNN controller hyperparameters are listed here. See
   // config_common.json for the full list.
   "controller" : {
      "learning_rate" : 0.001,
      "entropy_weight" : 0.01,
      "entropy_gamma" : 0.85
   },

   // Hyperparameters related to including in situ priors and constraints. Each
   // prior must explicitly be turned "on" or it will not be used. See
   // config_common.json for descriptions of each prior.
   "prior": {
      "length" : {
         "min_" : 4,
         "max_" : 30,
         "on" : true
      },
      "inverse" : {
         "on" : true
      },
      "trig" : {
         "on" : true
      },
      "const" : {
         "on" : true
      },
      "no_inputs" : {
         "on" : true
      },
      "uniform_arity" : {
         "on" : true
      },
      "soft_length" : {
         "loc" : 10,
         "scale" : 5,
         "on" : true
      }
   }
}
